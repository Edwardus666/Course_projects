---
title: "ConvirusData"
author: "Shunqi Zheng"
date: "2020/5/5"
output:
  word_document:
    toc: yes
  html_document: default
  pdf_document:
    toc: yes
---
```{r, echo=TRUE}
knitr::opts_chunk$set(error = TRUE)
```

the packages I installed:
```{r}
library(openxlsx)
library(xlsx)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(factoextra)
library(cluster)
```

All the data are sorted by date and grouped by location (country)
build a linear model

# Using the tool and problem met

I recognized that using a normal way to finsh a final project is a waste of time, for example, implement the code and make a report with text and images by copying and pasting. 

So I tried to google how to simplify this process. And I found there is a tool can achieve my thought: *Jupyter*. I put tons of time into configurating its installation, especially the IDE, because everytime I try to run the code in Python or in R, they didn't work since the External reasons. So I gave up this tool.

So I decided to find the alternative: *Rmarkdown*.
Then I can do images, words, and code at the same workplace.
It was so easy that I never thought I can simply work in Rstudio as ususal with some additional notation to add the text and images just like Word document.


# An so amazing idea when I Googling something(highlighted)

I was stuck on what functions I can apply for this coronavirus data,then
I googled it, I found many functions that they almost all have the pipe operators, "%>%, then I wonder why people always like using it in their codes.

Here is what an amazing thought I came across:

This is how my codes look like before in R:
plot(diff(log(sample(rnorm(10000, mean = 10, sd = 1), size = 100, replace = FALSE))), col = "red", type = "l")

I think this was a right case that I wrote the code, until the pipe operaters, %>%, someone using it.

let data =
    [|1..100|]
    |> Array.filter (fun i -> i*i <= 50)
    |> Array.map (fun i -> i+i*i)
    |> Array.sum

Yes, it is much clearer than the last same codes as they said.

When I tried to compare why my code is worse than this new one,
I suddenly recalled that I learned *Lambda calculus reduction* in math class.
That was so painful that I looking for which functions corresponding to which parameters. I was like why I have to learn how the old codes look like in modern 
era. 

Now until I learned how %>% parameter works, I realized that the problem has already solved by %>%. This is so cool operator simplify the code, which makes reader to read someone else's codes.

A great idea to solve parameters pass problem.

This is really what I found when doing this project.
I must note it down since it is an so amazing moment!



# Preprossing and analyzing the data:

First, let's read the data:
```{r}
country_data <-read.csv("country.csv")
world_data <- read.xlsx("world.xlsx", sheetIndex = 1)

```

Then let's see the summary of the dataset:

```{r}
summary(country_data)
```

```{r}
summary(world_data)
```
Although we know the date is from 2019-12-31 to 2020-04-08, however,
As I found that the date format is not in the same format

So I tried str() function

```{r}
str(country_data)
```

```{r}
str(world_data)

```

I found that the date format in country_data is "Factor" whereas in world_data is "Date".

And it is more clear when I use class() function to see the format of dates in two different dataset.

```{r}
class(country_data$date)

```

```{r}
class(world_data$date)
```

Then I used below function:
```{r}
country_data$date<- as.Date(country_data$date,format = "%m/%d/%Y")
```
Now, check the current format of it:
```{r}
class(country_data$date)
```

Finally, combine these two datasets into a whole one.

```{r}
virusData <- rbind(country_data,world_data)
```

Check the head and tail Data:
```{r}
head(virusData)
tail(virusData)
```


Then, let's check if the data has missing value:
```{r}
any(is.na(virusData))
```
Luckily, there is no missing value within it, so I can start applying the function now.

# Apply function

# # plot the data 

I use tidyverse package which includes many packages that used for plotting the data.
```{r}
# install.packages("tidyverse")
# library(tidyverse)

```



```{r}
par(mfrow=c(1,4))
plot(world_data$new_cases)
plot(world_data$total_cases)
plot(world_data$new_deaths)
plot(world_data$total_deaths)
```
We found that the world data points shown in different variables are all exponentially grows.

Let'see the country data:

```{r}
par(mfrow=c(1,4))
plot(country_data$new_cases)
plot(country_data$total_cases)
plot(country_data$new_deaths)
plot(country_data$total_deaths)

```
As we can see that these points are So messy, nothing special.

Now, let's try scatterpoint.
```{r}
# library(ggplot2)
ggplot(virusData, aes(x=date, y=total_cases, size=total_deaths,color=location))+geom_line()
```
it's too much to get the whole figure since the number of locations are too many.
```{r}
ggplot(virusData, aes(x=date, y=total_deaths, size=total_cases))+geom_line()
```

And it's much clear when I remove the function size() and color().
```{r}
library(dplyr)
virusData %>% ggplot(aes(date, total_cases))+geom_line()
```

```{r}
country_data %>%
  ggplot(aes(date, total_cases, group = location))+geom_line()
```
we can find a special curve that differs the others, but still not idea about which country it represents.

# # Build a linear regression
```{r}
linearmodel <- lm(total_cases ~total_deaths, data=virusData)
summary(linearmodel)

```

```{r}
ggplot(virusData,aes(x = total_deaths,y= total_cases))+geom_point()+stat_smooth(method = "lm", col="blue")
```
Here, we found that the line almost fit the data going further, but still, many other points are not.


let's just find out the feature of country and world data:
```{r}
par(mfrow=c(1,2))
# still make a linear model
linearmodel1 <- lm(total_cases ~total_deaths, data=country_data)
# plot the model
ggplot(country_data,aes(x = total_cases,y=total_deaths))+ggtitle("country_data") +geom_point()+stat_smooth(method = "lm", col="blue")

# for world data doing the same steps
linearmodel2 <- lm(total_cases ~total_deaths, data=world_data)
ggplot(world_data,aes(x = total_cases,y=total_deaths)) +geom_point()+stat_smooth(method = "lm", col="blue")+ggtitle("world_data")
```
So we found that the line is actually fit the world_data, not for the country data, I thought that the reason is different country should have their own linear model featursasdade.

# # # explore an idea

let's find out the top 10 countries with most cases within these months.
I was trying to select the top 10 countries in one specific date, like on 2020-2-15.

```{r}
subcountry_data <- filter(country_data,date == "2020-02-15")%>%
  arrange(.,total_cases)%>% tail(.,10)
subcountry_data
```
Ok, so let's do some investigation about this 10 countries.

```{r}
# Making a subset of selecting top 10 countries:
top10countries <- country_data[ country_data$location %in% c("United States","Vietnam","Taiwan","Malaysia","South Korea", "Thailand","Japan","Singapore","International","China"),]

# ggplot the subset with labels of locations
ggplot(top10countries,aes(x=date, y= total_cases, color = location,size=total_deaths))+geom_point()
```
We found that China and United States are the two countries that have more cases than the others. And China is the one with this different line we saw before.
the number of cases in United states is explosively risein the middle of March, whereas in China, it is tending to be flattening after the middle of March. They both have most cases during this period by the way.

```{r}
ggplot(top10countries,aes(x=date,y=total_cases,group = location,color = location, size = total_deaths))+geom_line()+facet_wrap(~location)
```
This is more clear.

```{r}
library(factoextra)
library(cluster)

par(mfrow=c(1,2))
fviz_nbclust(top10countries,FUN=hcut, method = "wss")
fviz_nbclust(top10countries,FUN=hcut, method = "silhouette")

```
```{r}
gap_stat<- clusGap(top10countries$total_cases, FUN=hcut,nstart = 25, K.max = 4, B=50)
```

```{r}
 top10countries$total_cases %>% na.omit() %>% scale()
k1<- kmeans(top10countries$total_cases, centers = 2, nstart = 25)
fviz_cluster(k1, data=top10countries)
```
I failed in this step.
And I still don't know why a dataframe with many vectors can be put in this cluster function, how the cluster function plot the the clusters to segragate datap based on too many vectors in a dataframe.



